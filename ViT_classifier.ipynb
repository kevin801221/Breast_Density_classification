{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevinluo/anaconda3/envs/yolov8/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 768])\n",
      "torch.Size([2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevinluo/anaconda3/envs/yolov8/lib/python3.9/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import ViTFeatureExtractor, ViTForImageClassification, TrainingArguments, Trainer, ViTConfig\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "# class VisionDataset(Dataset):\n",
    "#     def __init__(self, filepaths, labels, feature_extractor):\n",
    "#         self.filepaths = filepaths\n",
    "#         self.labels = labels\n",
    "#         self.feature_extractor = feature_extractor\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.filepaths)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         image = Image.open(self.filepaths[idx])\n",
    "#         image = np.array(image)\n",
    "#         encoding = self.feature_extractor(images=image, return_tensors='pt')\n",
    "#         encoding['pixel_values'] = encoding['pixel_values'].squeeze() # remove batch dimension\n",
    "#         label = self.labels[idx]\n",
    "#         return {'pixel_values': encoding['pixel_values'], 'label': torch.tensor(label)}\n",
    "\n",
    "class VisionDataset(Dataset):\n",
    "    def __init__(self, filepaths, labels, feature_extractor):\n",
    "        self.filepaths = filepaths\n",
    "        self.labels = labels\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.transform = transforms.Compose([\n",
    "            # other transforms...\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filepaths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.filepaths[idx])\n",
    "        image = image * 0.5 + 0.5  # unnormalize\n",
    "        image = Image.fromarray((image * 255).astype(np.uint8))  # convert to PIL image\n",
    "        image = self.transform(image)\n",
    "        if len(image.shape) == 2:\n",
    "            image = image.unsqueeze(0)\n",
    "        encoding = self.feature_extractor(images=image, return_tensors='pt')\n",
    "        encoding['pixel_values'] = encoding['pixel_values'].squeeze() # remove batch dimension\n",
    "        label = self.labels[idx]\n",
    "        return {'pixel_values': encoding['pixel_values'], 'label': torch.tensor(label)}\n",
    "\n",
    "# load pretrained vision transformer model and feature extractor\n",
    "config = ViTConfig.from_pretrained('google/vit-base-patch16-224')\n",
    "config.num_labels = 2\n",
    "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224', config=config,ignore_mismatched_sizes=True)\n",
    "# Classifier's weight and bias sizes should now match with the updated config.\n",
    "print(model.classifier.weight.size())\n",
    "print(model.classifier.bias.size())\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevinluo/anaconda3/envs/yolov8/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'JpegImageFile' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 48\u001b[0m\n\u001b[1;32m     40\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     41\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m     42\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[1;32m     43\u001b[0m     train_dataset\u001b[39m=\u001b[39mtrain_dataset,\n\u001b[1;32m     44\u001b[0m     eval_dataset\u001b[39m=\u001b[39mval_dataset,\n\u001b[1;32m     45\u001b[0m )\n\u001b[1;32m     47\u001b[0m \u001b[39m# train and validate\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m     49\u001b[0m eval_result \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mevaluate()\n\u001b[1;32m     51\u001b[0m \u001b[39m# save model\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/yolov8/lib/python3.9/site-packages/transformers/trainer.py:1645\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1640\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1642\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1643\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1644\u001b[0m )\n\u001b[0;32m-> 1645\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1646\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1647\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1648\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1649\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1650\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/yolov8/lib/python3.9/site-packages/transformers/trainer.py:1916\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1913\u001b[0m     rng_to_sync \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1915\u001b[0m step \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m-> 1916\u001b[0m \u001b[39mfor\u001b[39;00m step, inputs \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   1917\u001b[0m     total_batched_samples \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1918\u001b[0m     \u001b[39mif\u001b[39;00m rng_to_sync:\n",
      "File \u001b[0;32m~/anaconda3/envs/yolov8/lib/python3.9/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/yolov8/lib/python3.9/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/yolov8/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/yolov8/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[1], line 42\u001b[0m, in \u001b[0;36mVisionDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, idx):\n\u001b[1;32m     41\u001b[0m     image \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilepaths[idx])\n\u001b[0;32m---> 42\u001b[0m     image \u001b[39m=\u001b[39m image \u001b[39m*\u001b[39;49m \u001b[39m0.5\u001b[39;49m \u001b[39m+\u001b[39m \u001b[39m0.5\u001b[39m  \u001b[39m# unnormalize\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     image \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray((image \u001b[39m*\u001b[39m \u001b[39m255\u001b[39m)\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39muint8))  \u001b[39m# convert to PIL image\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(image)\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'JpegImageFile' and 'float'"
     ]
    }
   ],
   "source": [
    "# prepare data\n",
    "dir_path = '/home/kevinluo/Benign_Malignant_dataclassifier/'\n",
    "subfolders = ['train', 'valid']\n",
    "labels_dict = {'benign': 0, 'malignant': 1}\n",
    "all_files = []\n",
    "all_labels = []\n",
    "\n",
    "for subfolder in subfolders:\n",
    "    for label_folder, label_id in labels_dict.items():\n",
    "        folder_path = os.path.join(dir_path, subfolder, label_folder)\n",
    "        files = [os.path.join(folder_path, f) for f in os.listdir(folder_path)]\n",
    "        labels = [label_id]*len(files)\n",
    "        all_files.extend(files)\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "# split data into 10 folds\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "results = {}\n",
    "\n",
    "for fold, (train_ids, val_ids) in enumerate(skf.split(all_files, all_labels)):\n",
    "    # prepare train and val datasets\n",
    "    train_dataset = VisionDataset([all_files[i] for i in train_ids], [all_labels[i] for i in train_ids], feature_extractor)\n",
    "    val_dataset = VisionDataset([all_files[i] for i in val_ids], [all_labels[i] for i in val_ids], feature_extractor)\n",
    "    \n",
    "    # set up training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f'./results_fold_{fold}',\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=64,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=10,\n",
    "        evaluation_strategy='steps',\n",
    "        save_strategy='steps'\n",
    "    )\n",
    "    \n",
    "    # initialize the trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "    )\n",
    "    \n",
    "    # train and validate\n",
    "    trainer.train()\n",
    "    eval_result = trainer.evaluate()\n",
    "    \n",
    "    # save model\n",
    "    trainer.save_model(f'./results_fold_{fold}/model')\n",
    "    \n",
    "    # store results\n",
    "    results[fold] = eval_result\n",
    "\n",
    "# print results\n",
    "for fold, result in results.items():\n",
    "    print(f\"Fold: {fold}, Eval Result: {result}\")\n",
    "\n",
    "# test data\n",
    "test_files = []\n",
    "test_labels = []\n",
    "for label_folder, label_id in labels_dict.items():\n",
    "    folder_path = os.path.join(dir_path, 'test', label_folder)\n",
    "    files = [os.path.join(folder_path, f) for f in os.listdir(folder_path)]\n",
    "    labels = [label_id]*len(files)\n",
    "    test_files.extend(files)\n",
    "    test_labels.extend(labels)\n",
    "\n",
    "# test dataset\n",
    "test_dataset = VisionDataset(test_files, test_labels, feature_extractor)\n",
    "\n",
    "# load the best model and make prediction\n",
    "best_model_index = np.argmin([result['eval_loss'] for result in results.values()])\n",
    "model = ViTForImageClassification.from_pretrained(f'./results_fold_{best_model_index}/model')\n",
    "trainer = Trainer(model=model, args=training_args)\n",
    "predictions = trainer.predict(test_dataset=test_dataset)\n",
    "\n",
    "# print prediction result\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevinluo/anaconda3/envs/yolov8/lib/python3.9/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "MlflowException",
     "evalue": "Experiment 'Fold_0' already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMlflowException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 97\u001b[0m\n\u001b[1;32m     94\u001b[0m valid_images, valid_labels \u001b[39m=\u001b[39m [images[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m valid_index], [labels[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m valid_index]\n\u001b[1;32m     96\u001b[0m \u001b[39m# Create a new experiment for each fold\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m mlflow\u001b[39m.\u001b[39;49mcreate_experiment(name\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mFold_\u001b[39;49m\u001b[39m{\u001b[39;49;00mfold\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     99\u001b[0m \u001b[39m# Then, start a new run in the newly created experiment\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[39mwith\u001b[39;00m mlflow\u001b[39m.\u001b[39mstart_run(experiment_id\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFold_\u001b[39m\u001b[39m{\u001b[39;00mfold\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/yolov8/lib/python3.9/site-packages/mlflow/tracking/fluent.py:1429\u001b[0m, in \u001b[0;36mcreate_experiment\u001b[0;34m(name, artifact_location, tags)\u001b[0m\n\u001b[1;32m   1384\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_experiment\u001b[39m(\n\u001b[1;32m   1385\u001b[0m     name: \u001b[39mstr\u001b[39m,\n\u001b[1;32m   1386\u001b[0m     artifact_location: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1387\u001b[0m     tags: Optional[Dict[\u001b[39mstr\u001b[39m, Any]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1388\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m   1389\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1390\u001b[0m \u001b[39m    Create an experiment.\u001b[39;00m\n\u001b[1;32m   1391\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1427\u001b[0m \u001b[39m        Creation timestamp: 1662004217511\u001b[39;00m\n\u001b[1;32m   1428\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1429\u001b[0m     \u001b[39mreturn\u001b[39;00m MlflowClient()\u001b[39m.\u001b[39;49mcreate_experiment(name, artifact_location, tags)\n",
      "File \u001b[0;32m~/anaconda3/envs/yolov8/lib/python3.9/site-packages/mlflow/tracking/client.py:557\u001b[0m, in \u001b[0;36mMlflowClient.create_experiment\u001b[0;34m(self, name, artifact_location, tags)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_experiment\u001b[39m(\n\u001b[1;32m    510\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    511\u001b[0m     name: \u001b[39mstr\u001b[39m,\n\u001b[1;32m    512\u001b[0m     artifact_location: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    513\u001b[0m     tags: Optional[Dict[\u001b[39mstr\u001b[39m, Any]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    514\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m    515\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Create an experiment.\u001b[39;00m\n\u001b[1;32m    516\u001b[0m \n\u001b[1;32m    517\u001b[0m \u001b[39m    :param name: The experiment name. Must be unique.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[39m        Lifecycle_stage: active\u001b[39;00m\n\u001b[1;32m    556\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 557\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tracking_client\u001b[39m.\u001b[39;49mcreate_experiment(name, artifact_location, tags)\n",
      "File \u001b[0;32m~/anaconda3/envs/yolov8/lib/python3.9/site-packages/mlflow/tracking/_tracking_service/client.py:236\u001b[0m, in \u001b[0;36mTrackingServiceClient.create_experiment\u001b[0;34m(self, name, artifact_location, tags)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Create an experiment.\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \n\u001b[1;32m    227\u001b[0m \u001b[39m:param name: The experiment name. Must be unique.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[39m:return: Integer ID of the created experiment.\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    234\u001b[0m _validate_experiment_artifact_location(artifact_location)\n\u001b[0;32m--> 236\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstore\u001b[39m.\u001b[39;49mcreate_experiment(\n\u001b[1;32m    237\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m    238\u001b[0m     artifact_location\u001b[39m=\u001b[39;49martifact_location,\n\u001b[1;32m    239\u001b[0m     tags\u001b[39m=\u001b[39;49m[ExperimentTag(key, value) \u001b[39mfor\u001b[39;49;00m (key, value) \u001b[39min\u001b[39;49;00m tags\u001b[39m.\u001b[39;49mitems()] \u001b[39mif\u001b[39;49;00m tags \u001b[39melse\u001b[39;49;00m [],\n\u001b[1;32m    240\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/yolov8/lib/python3.9/site-packages/mlflow/store/tracking/file_store.py:376\u001b[0m, in \u001b[0;36mFileStore.create_experiment\u001b[0;34m(self, name, artifact_location, tags)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_root_dir()\n\u001b[1;32m    375\u001b[0m _validate_experiment_name(name)\n\u001b[0;32m--> 376\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_experiment_does_not_exist(name)\n\u001b[1;32m    377\u001b[0m experiment_id \u001b[39m=\u001b[39m _generate_unique_integer_id()\n\u001b[1;32m    378\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_experiment_with_id(name, \u001b[39mstr\u001b[39m(experiment_id), artifact_location, tags)\n",
      "File \u001b[0;32m~/anaconda3/envs/yolov8/lib/python3.9/site-packages/mlflow/store/tracking/file_store.py:368\u001b[0m, in \u001b[0;36mFileStore._validate_experiment_does_not_exist\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[39mraise\u001b[39;00m MlflowException(\n\u001b[1;32m    361\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mExperiment \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m already exists in deleted state. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    362\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou can restore the experiment, or permanently delete the experiment \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    365\u001b[0m         databricks_pb2\u001b[39m.\u001b[39mRESOURCE_ALREADY_EXISTS,\n\u001b[1;32m    366\u001b[0m     )\n\u001b[1;32m    367\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 368\u001b[0m     \u001b[39mraise\u001b[39;00m MlflowException(\n\u001b[1;32m    369\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mExperiment \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m already exists.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m experiment\u001b[39m.\u001b[39mname,\n\u001b[1;32m    370\u001b[0m         databricks_pb2\u001b[39m.\u001b[39mRESOURCE_ALREADY_EXISTS,\n\u001b[1;32m    371\u001b[0m     )\n",
      "\u001b[0;31mMlflowException\u001b[0m: Experiment 'Fold_0' already exists."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet50\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import ViTFeatureExtractor, ViTForImageClassification, TrainingArguments, Trainer\n",
    "import mlflow\n",
    "# 資料夾路徑\n",
    "directories = [\n",
    "    '/home/kevinluo/Benign_Malignant_dataclassifier/train/benign',\n",
    "    '/home/kevinluo/Benign_Malignant_dataclassifier/train/malignant',\n",
    "    '/home/kevinluo/Benign_Malignant_dataclassifier/valid/benign',\n",
    "    '/home/kevinluo/Benign_Malignant_dataclassifier/valid/malignant',\n",
    "]\n",
    "\n",
    "# 整理資料與標籤\n",
    "images = []\n",
    "labels = []\n",
    "for i, directory in enumerate(directories):\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.jpg'):\n",
    "            images.append(os.path.join(directory, filename))\n",
    "            labels.append(i // 2)  # 為 benign 資料夾給 0，malignant 資料夾給 1\n",
    "\n",
    "# feature extractor\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n",
    "\n",
    "class MammographyDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transforms=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "        label = self.labels[idx]\n",
    "        return image, label\n",
    "\n",
    "# 訓練函數\n",
    "def train_and_validate(train_images, train_labels, valid_images, valid_labels):\n",
    "    # 建立 dataloader\n",
    "    train_dataset = MammographyDataset(train_images, train_labels, transforms=transforms.ToTensor())\n",
    "    valid_dataset = MammographyDataset(valid_images, valid_labels, transforms=transforms.ToTensor())\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    valid_dataloader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    # 建立模型\n",
    "    model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224', num_labels=2, ignore_mismatched_sizes=True)\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    # 設定訓練參數\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=64,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "    )\n",
    "\n",
    "    # 建立 trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=valid_dataset,\n",
    "    )\n",
    "\n",
    "    # 訓練模型\n",
    "    trainer.train()\n",
    "\n",
    "    # 驗證模型\n",
    "    metrics = trainer.evaluate()\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# 進行 10-fold cross-validation\n",
    "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
    "results = []\n",
    "for fold, (train_index, valid_index) in enumerate(kf.split(images, labels)):\n",
    "    train_images, train_labels = [images[i] for i in train_index], [labels[i] for i in train_index]\n",
    "    valid_images, valid_labels = [images[i] for i in valid_index], [labels[i] for i in valid_index]\n",
    "    \n",
    "    # Create a new experiment for each fold\n",
    "    mlflow.create_experiment(name=f\"Fold_{fold}\")\n",
    "    \n",
    "    # Then, start a new run in the newly created experiment\n",
    "    with mlflow.start_run(experiment_id=f\"Fold_{fold}\"):\n",
    "        result = train_and_validate(train_images, train_labels, valid_images, valid_labels, output_dir=f'./results_fold_{fold}')\n",
    "        results.append(result)\n",
    "\n",
    "# 輸出結果\n",
    "for i, result in enumerate(results):\n",
    "    print(f'Fold {i+1}:')\n",
    "    print(f'\\tLoss: {result[\"eval_loss\"]}')\n",
    "    print(f'\\tAccuracy: {result[\"eval_accuracy\"]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevinluo/anaconda3/envs/yolov8/lib/python3.9/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ViTForImageClassification:\n\tsize mismatch for classifier.weight: copying a param with shape torch.Size([1000, 768]) from checkpoint, the shape in current model is torch.Size([2, 768]).\n\tsize mismatch for classifier.bias: copying a param with shape torch.Size([1000]) from checkpoint, the shape in current model is torch.Size([2]).\n\tYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 128\u001b[0m\n\u001b[1;32m    126\u001b[0m     train_images, train_labels \u001b[39m=\u001b[39m [train_valid_images[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m train_index], [train_valid_labels[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m train_index]\n\u001b[1;32m    127\u001b[0m     valid_images, valid_labels \u001b[39m=\u001b[39m [train_valid_images[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m valid_index], [train_valid_labels[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m valid_index]\n\u001b[0;32m--> 128\u001b[0m     train_and_validate(train_images, train_labels, valid_images, valid_labels)\n\u001b[1;32m    130\u001b[0m \u001b[39m# 最後使用全部的 train 和 valid 數據來對不起，我不小心送出了不完整的回答，以下是完整的程式碼：\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \n\u001b[1;32m    132\u001b[0m \n\u001b[1;32m    133\u001b[0m \u001b[39m# 最後使用全部的 train 和 valid 數據來訓練模型，並在 test 數據上進行測試\u001b[39;00m\n\u001b[1;32m    134\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m torch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[13], line 99\u001b[0m, in \u001b[0;36mtrain_and_validate\u001b[0;34m(train_images, train_labels, valid_images, valid_labels)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_and_validate\u001b[39m(train_images, train_labels, valid_images, valid_labels):\n\u001b[1;32m     97\u001b[0m     device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m torch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 99\u001b[0m     model \u001b[39m=\u001b[39m ViTForImageClassification\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m'\u001b[39;49m\u001b[39mgoogle/vit-base-patch16-224\u001b[39;49m\u001b[39m'\u001b[39;49m, num_labels\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[1;32m    100\u001b[0m     model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m    102\u001b[0m     train_dataset \u001b[39m=\u001b[39m CustomImageDataset(train_images, train_labels, transform\u001b[39m=\u001b[39mtransforms\u001b[39m.\u001b[39mCompose([transforms\u001b[39m.\u001b[39mResize((\u001b[39m224\u001b[39m, \u001b[39m224\u001b[39m)), transforms\u001b[39m.\u001b[39mToTensor()]))\n",
      "File \u001b[0;32m~/anaconda3/envs/yolov8/lib/python3.9/site-packages/transformers/modeling_utils.py:2881\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2871\u001b[0m     \u001b[39mif\u001b[39;00m dtype_orig \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2872\u001b[0m         torch\u001b[39m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   2874\u001b[0m     (\n\u001b[1;32m   2875\u001b[0m         model,\n\u001b[1;32m   2876\u001b[0m         missing_keys,\n\u001b[1;32m   2877\u001b[0m         unexpected_keys,\n\u001b[1;32m   2878\u001b[0m         mismatched_keys,\n\u001b[1;32m   2879\u001b[0m         offload_index,\n\u001b[1;32m   2880\u001b[0m         error_msgs,\n\u001b[0;32m-> 2881\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_load_pretrained_model(\n\u001b[1;32m   2882\u001b[0m         model,\n\u001b[1;32m   2883\u001b[0m         state_dict,\n\u001b[1;32m   2884\u001b[0m         loaded_state_dict_keys,  \u001b[39m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   2885\u001b[0m         resolved_archive_file,\n\u001b[1;32m   2886\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   2887\u001b[0m         ignore_mismatched_sizes\u001b[39m=\u001b[39;49mignore_mismatched_sizes,\n\u001b[1;32m   2888\u001b[0m         sharded_metadata\u001b[39m=\u001b[39;49msharded_metadata,\n\u001b[1;32m   2889\u001b[0m         _fast_init\u001b[39m=\u001b[39;49m_fast_init,\n\u001b[1;32m   2890\u001b[0m         low_cpu_mem_usage\u001b[39m=\u001b[39;49mlow_cpu_mem_usage,\n\u001b[1;32m   2891\u001b[0m         device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m   2892\u001b[0m         offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[1;32m   2893\u001b[0m         offload_state_dict\u001b[39m=\u001b[39;49moffload_state_dict,\n\u001b[1;32m   2894\u001b[0m         dtype\u001b[39m=\u001b[39;49mtorch_dtype,\n\u001b[1;32m   2895\u001b[0m         is_quantized\u001b[39m=\u001b[39;49m(load_in_8bit \u001b[39mor\u001b[39;49;00m load_in_4bit),\n\u001b[1;32m   2896\u001b[0m         keep_in_fp32_modules\u001b[39m=\u001b[39;49mkeep_in_fp32_modules,\n\u001b[1;32m   2897\u001b[0m     )\n\u001b[1;32m   2899\u001b[0m model\u001b[39m.\u001b[39mis_loaded_in_4bit \u001b[39m=\u001b[39m load_in_4bit\n\u001b[1;32m   2900\u001b[0m model\u001b[39m.\u001b[39mis_loaded_in_8bit \u001b[39m=\u001b[39m load_in_8bit\n",
      "File \u001b[0;32m~/anaconda3/envs/yolov8/lib/python3.9/site-packages/transformers/modeling_utils.py:3278\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   3274\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39msize mismatch\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m error_msg:\n\u001b[1;32m   3275\u001b[0m         error_msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[1;32m   3276\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39mYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3277\u001b[0m         )\n\u001b[0;32m-> 3278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{\u001b[39;00merror_msg\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   3280\u001b[0m \u001b[39mif\u001b[39;00m is_quantized:\n\u001b[1;32m   3281\u001b[0m     unexpected_keys \u001b[39m=\u001b[39m [elem \u001b[39mfor\u001b[39;00m elem \u001b[39min\u001b[39;00m unexpected_keys \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mSCB\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m elem]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ViTForImageClassification:\n\tsize mismatch for classifier.weight: copying a param with shape torch.Size([1000, 768]) from checkpoint, the shape in current model is torch.Size([2, 768]).\n\tsize mismatch for classifier.bias: copying a param with shape torch.Size([1000]) from checkpoint, the shape in current model is torch.Size([2]).\n\tYou may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import ViTForImageClassification, AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 資料夾路徑\n",
    "directories = [\n",
    "    '/home/kevinluo/Benign_Malignant_dataclassifier/train/benign',\n",
    "    '/home/kevinluo/Benign_Malignant_dataclassifier/train/malignant',\n",
    "    '/home/kevinluo/Benign_Malignant_dataclassifier/valid/benign',\n",
    "    '/home/kevinluo/Benign_Malignant_dataclassifier/valid/malignant',\n",
    "]\n",
    "\n",
    "# 整理資料與標籤\n",
    "images = []\n",
    "labels = []\n",
    "for i, directory in enumerate(directories):\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.jpg'):\n",
    "            images.append(os.path.join(directory, filename))\n",
    "            labels.append(i // 2)  # 為 benign 資料夾給 0，malignant 資料夾給 1\n",
    "# feature extractor\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n",
    "\n",
    "# 定義我們的 dataset\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, img_paths, labels, transform=None):\n",
    "        self.img_paths = img_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# 定義我們的 train function\n",
    "def train(model, device, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(tqdm(train_loader, desc='Training')):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output.logits, target)\n",
    "        train_loss += loss.item()\n",
    "        pred = output.logits.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return train_loss / len(train_loader), correct / len(train_loader.dataset)\n",
    "\n",
    "# 定義我們的 validation function\n",
    "def validate(model, device, valid_loader, criterion):\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(valid_loader, desc='Validating'):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            valid_loss += criterion(output.logits, target).item()\n",
    "            pred = output.logits.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    return valid_loss / len(valid_loader), correct / len(valid_loader.dataset)\n",
    "\n",
    "# 定義我們的 test function\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(test_loader, desc='Testing'):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.logits.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    return correct / len(test_loader.dataset)\n",
    "\n",
    "# 定義一個 function 來執行我們的 10-fold validation 訓練和驗證\n",
    "def train_and_validate(train_images, train_labels, valid_images, valid_labels):\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    \n",
    "    model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224', num_labels=2)\n",
    "    model = model.to(device)\n",
    "\n",
    "    train_dataset = CustomImageDataset(train_images, train_labels, transform=transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()]))\n",
    "    valid_dataset = CustomImageDataset(valid_images, valid_labels, transform=transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()]))\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "    \n",
    "    for epoch in range(10):  # 這裡我們只訓練 10 個 epoch\n",
    "        train_loss, train_acc = train(model, device, train_loader, criterion, optimizer)\n",
    "        valid_loss, valid_acc = validate(model, device, valid_loader, criterion)\n",
    "        print(f'Epoch: {epoch}, Train Loss: {train_loss}, Train Acc: {train_acc}, Valid Loss: {valid_loss}, Valid Acc: {valid_acc}')\n",
    "\n",
    "    torch.save(model.state_dict(), f'./vit_model_fold.pth')\n",
    "\n",
    "# 先將 train 和 valid 的路徑及 label 合併\n",
    "train_valid_images = train_images + valid_images\n",
    "train_valid_labels = train_labels + valid_labels\n",
    "\n",
    "# 進行 10-fold cross-validation\n",
    "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
    "results = []\n",
    "for fold, (train_index, valid_index) in enumerate(kf.split(train_valid_images, train_valid_labels)):\n",
    "    train_images, train_labels = [train_valid_images[i] for i in train_index], [train_valid_labels[i] for i in train_index]\n",
    "    valid_images, valid_labels = [train_valid_images[i] for i in valid_index], [train_valid_labels[i] for i in valid_index]\n",
    "    train_and_validate(train_images, train_labels, valid_images, valid_labels)\n",
    "    \n",
    "# 最後使用全部的 train 和 valid 數據來對不起，我不小心送出了不完整的回答，以下是完整的程式碼：\n",
    "\n",
    "\n",
    "# 最後使用全部的 train 和 valid 數據來訓練模型，並在 test 數據上進行測試\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224', num_labels=2)\n",
    "model = model.to(device)\n",
    "\n",
    "train_valid_dataset = CustomImageDataset(train_valid_images, train_valid_labels, transform=transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()]))\n",
    "test_dataset = CustomImageDataset(test_images, test_labels, transform=transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()]))\n",
    "\n",
    "train_valid_loader = DataLoader(train_valid_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "for epoch in range(10):  # 這裡我們只訓練 10 個 epoch\n",
    "    train_loss, train_acc = train(model, device, train_valid_loader, criterion, optimizer)\n",
    "    print(f'Epoch: {epoch}, Train Loss: {train_loss}, Train Acc: {train_acc}')\n",
    "\n",
    "test_acc = test(model, device, test_loader)\n",
    "print(f'Test Acc: {test_acc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTModel\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ViTForImageClassification(nn.Module):\n",
    "    def __init__(self, num_labels=3):\n",
    "        super(ViTForImageClassification, self).__init__()\n",
    "        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(self.vit.config.hidden_size, num_labels)\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "    def forward(self, pixel_values, labels):\n",
    "        outputs = self.vit(pixel_values=pixel_values)\n",
    "        output = self.dropout(outputs.last_hidden_state[:,0])\n",
    "        logits = self.classifier(output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "          loss_fct = nn.CrossEntropyLoss()\n",
    "          loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        if loss is not None:\n",
    "          return logits, loss.item()\n",
    "        else:\n",
    "          return logits, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTFeatureExtractor\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "# Define Model\n",
    "#model = ViTForImageClassification(len(train_ds.classes))\n",
    "# Feature Extractor\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "# Adam Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "# Cross Entropy Loss\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "# Use GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "  model.cuda()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
